{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_tick_range(values, interval=20):\n",
    "    start = np.floor(min(values) / interval) * interval\n",
    "    end = np.ceil(max(values) / interval) * interval\n",
    "    return np.arange(start, end + 1, interval)\n",
    "\n",
    "\n",
    "# Create a function to save plots\n",
    "def save_plot(plt, filename, output_dir):\n",
    "    if not os.path.exists(output_dir):  # Check if the output directory exists, create it if not\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "\n",
    "def plot_average_episode_rewards(average_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Episode Reward')\n",
    "    plt.title(f'Average Episode Reward Progress - {scenario}')\n",
    "    plt.plot(range(1, len(average_rewards) + 1), average_rewards)\n",
    "    plt.yticks(calculate_tick_range(average_rewards))\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Average_Episode_Reward_Progress_{scenario}.png', output_dir)\n",
    "    \n",
    "def plot_average_episode_rewards_rolling(average_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Episode Reward')\n",
    "    plt.title(f'Average Episode Reward Progress - {scenario}')\n",
    "    \n",
    "    plt.plot(range(1, len(average_rewards) + 1), average_rewards, alpha=0.3, label='Original')\n",
    "    \n",
    "    rewards_series = pd.Series(average_rewards)\n",
    "    smoothed_rewards = rewards_series.rolling(window=100).mean()\n",
    "\n",
    "    plt.plot(range(1, len(average_rewards) + 1), smoothed_rewards, color='red', label='Smoothed (Rolling Mean)')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(smoothed_rewards.dropna()))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Average_Episode_Reward_Progress_Rolling_Mean{scenario}.png', output_dir)\n",
    "\n",
    "def plot_all_agents_rewards(agent_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Agent Reward')\n",
    "    plt.title(f'All Agents Reward Progress (agent + avdversary) - {scenario}')\n",
    "    \n",
    "    all_rewards = np.concatenate([rewards for rewards in agent_rewards.values()])\n",
    "    for agent_name, rewards in agent_rewards.items():\n",
    "        plt.plot(range(1, len(rewards) + 1), rewards, label=f'Agent {agent_name}')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(all_rewards))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'All_Agents_Reward_Progress_{scenario}.png', output_dir)\n",
    "\n",
    "\n",
    "def smooth_data(data, window_size):\n",
    "    \"\"\"Apply a moving average to smooth the data.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "\n",
    "\n",
    "def plot_all_agents_rewards_smooth(agent_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Agent Reward')\n",
    "    plt.title(f'All Agents Reward Progress (agent + avdversary) - {scenario} smooth')\n",
    "    \n",
    "    all_rewards = np.concatenate([rewards for rewards in agent_rewards.values()])\n",
    "    for agent_name, rewards in agent_rewards.items():\n",
    "        plot_data = smooth_data( rewards, window_size=100)\n",
    "        plt.plot(range(1, len(plot_data) + 1),plot_data, label=f'Agent {agent_name}')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(all_rewards))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'All_Agents_Reward_Progress_{scenario}_smooth.png', output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_individual_agent_rewards(epsiode_mean_agent_rewards, agent_name, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Mean Agent Reward')\n",
    "    plt.title(f'Mean Agent Reward Progress - {scenario} (Agent {agent_name})')\n",
    "    plt.plot(range(1, len(epsiode_mean_agent_rewards) + 1), epsiode_mean_agent_rewards, label=f'Agent {agent_name}')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(epsiode_mean_agent_rewards))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Individual_Agent_Reward_Progress_{scenario}_Agent_{agent_name}.png', output_dir)\n",
    "\n",
    "def plot_everything(output_dir, scenario, k, score_history_100, score_history, epsiode_mean_agent_rewards):\n",
    "    # Plot results for different subpolicies\n",
    "            output_subdir = os.path.join(output_dir, f'scenario_{scenario}', f'k_{k}')\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "            # Plot for average episode rewards fancy\n",
    "            plot_average_episode_rewards(score_history_100, f\"{scenario} - {k}\", output_subdir)\n",
    "\n",
    "            # Plot for average episode rewards with rolling mean\n",
    "            plot_average_episode_rewards_rolling(score_history, f\"{scenario} - {k}\", output_subdir)\n",
    "            \n",
    "            # Plot for individual agent rewards\n",
    "            plot_all_agents_rewards(epsiode_mean_agent_rewards, f\"{scenario} - {k}\", output_subdir)\n",
    "            \n",
    "            # Plot for 'agent_0' only\n",
    "            agent_name = 'agent_0'\n",
    "            if agent_name in epsiode_mean_agent_rewards:\n",
    "                plot_individual_agent_rewards(epsiode_mean_agent_rewards[agent_name], agent_name, f\"{scenario} - {k}\", output_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "        self.fc1 = nn.Linear(input_dims+n_actions, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.q = nn.Linear(fc2_dims, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = T.sigmoid(self.pi(x))\n",
    "\n",
    "        return pi\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.chkpt_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.chkpt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha, beta, input_dims, tau, n_actions, gamma=0.99,\n",
    "                 max_size=1000000, fc1_dims=400, fc2_dims=300,\n",
    "                 batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                                  n_actions=n_actions, name='actor')\n",
    "        self.critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
    "                                    n_actions=n_actions, name='critic')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                                         n_actions=n_actions,\n",
    "                                         name='target_actor')\n",
    "\n",
    "        self.target_critic = CriticNetwork(beta, input_dims, fc1_dims,\n",
    "                                           fc2_dims, n_actions=n_actions,\n",
    "                                           name='target_critic')\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation, eval=False):\n",
    "        state = T.tensor(observation[np.newaxis, :], dtype=T.float,\n",
    "                         device=self.actor.device)\n",
    "        mu = self.actor.forward(state).to(self.actor.device)\n",
    "        noise = T.rand(self.n_actions).to(self.actor.device)\n",
    "        noise *= T.tensor(1 - int(eval))\n",
    "        mu_prime = mu + noise\n",
    "        mu_prime = T.clamp(mu_prime, 0., 1.)\n",
    "\n",
    "        return mu_prime.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_, done = \\\n",
    "            self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(states, dtype=T.float).to(self.actor.device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(self.actor.device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(self.actor.device)\n",
    "        rewards = T.tensor(rewards, dtype=T.float).to(self.actor.device)\n",
    "        done = T.tensor(done).to(self.actor.device)\n",
    "\n",
    "        target_actions = self.target_actor.forward(states_)\n",
    "        critic_value_ = self.target_critic.forward(states_, target_actions)\n",
    "        critic_value = self.critic.forward(states, actions)\n",
    "\n",
    "        critic_value_[done] = 0.0\n",
    "        critic_value_ = critic_value_.view(-1)\n",
    "\n",
    "        target = rewards + self.gamma*critic_value_\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss = -self.critic.forward(states, self.actor.forward(states))\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        tau = tau or self.tau\n",
    "        src = self.actor\n",
    "        dest = self.target_actor\n",
    "        for param, target in zip(src.parameters(), dest.parameters()):\n",
    "            target.data.copy_(tau * param.data + (1 - tau) * target.data)\n",
    "        src = self.critic\n",
    "        dest = self.target_critic\n",
    "        for param, target in zip(src.parameters(), dest.parameters()):\n",
    "            target.data.copy_(tau * param.data + (1 - tau) * target.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def obs_list_to_state_vector(observation):\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state\n",
    "\n",
    "\n",
    "def smooth_data(data, window_size):\n",
    "    \"\"\"Apply a moving average to smooth the data.\"\"\"\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "\n",
    "def train_DDPG(parallel_env, N_GAMES, scenario, output_dir):\n",
    "    _, _ = parallel_env.reset()\n",
    "    n_agents = parallel_env.max_num_agents\n",
    "\n",
    "    agents = []\n",
    "    for agent in parallel_env.agents:\n",
    "        input_dims = parallel_env.observation_space(agent).shape[0]\n",
    "        n_actions = parallel_env.action_space(agent).shape[0]\n",
    "\n",
    "        agents.append(Agent(input_dims=input_dims, n_actions=n_actions,\n",
    "                            gamma=0.95, tau=0.01, alpha=1e-4, beta=1e-3))\n",
    "\n",
    "    EVAL_INTERVAL = 1000\n",
    "    MAX_STEPS = N_GAMES * 25  # 25 steps per episode\n",
    "    total_steps = 0\n",
    "    episode = 0\n",
    "\n",
    "    episode_rewards = []  # Store rewards for each episode\n",
    "    epsiode_mean_agent_rewards = {agent_name: [] for agent_name in parallel_env.agents}\n",
    "    eval_scores = []\n",
    "    eval_steps = []\n",
    "    score = evaluate(agents, parallel_env, episode, total_steps)\n",
    "    eval_scores.append(score)\n",
    "    eval_steps.append(total_steps)\n",
    "\n",
    "    pbar = tqdm(total=MAX_STEPS, desc=\"Training DDPG\")\n",
    "\n",
    "    while total_steps < MAX_STEPS:\n",
    "        obs, _ = parallel_env.reset()\n",
    "        terminal = [False] * n_agents\n",
    "        obs = list(obs.values())\n",
    "        episode_reward = 0\n",
    "        agent_rewards = {agent_name: [] for agent_name in parallel_env.agents}\n",
    "        while not any(terminal):\n",
    "            action = [agent.choose_action(obs[idx])\n",
    "                      for idx, agent in enumerate(agents)]\n",
    "            action = {agent: act\n",
    "                      for agent, act in zip(parallel_env.agents, action)}\n",
    "            obs_, reward, done, truncated, info = parallel_env.step(action)\n",
    "            list_done = list(done.values())\n",
    "            list_reward = list(reward.values())\n",
    "            list_action = list(action.values())\n",
    "            obs_ = list(obs_.values())\n",
    "            list_trunc = list(truncated.values())\n",
    "\n",
    "            terminal = [d or t for d, t in zip(list_done, list_trunc)]\n",
    "\n",
    "            for idx, agent in enumerate(agents):\n",
    "                agent.remember(obs[idx], list_action[idx],\n",
    "                               list_reward[idx], obs_[idx], terminal[idx])\n",
    "\n",
    "            if total_steps % 125 == 0:\n",
    "                for agent in agents:\n",
    "                    agent.learn()\n",
    "            obs = obs_\n",
    "            print(reward)\n",
    "            # Store the rewards\n",
    "            for agent_name, r in reward.items():\n",
    "                agent_rewards[agent_name].append(r)\n",
    "            input(\"Press Enter to continue...\")\n",
    "\n",
    "            episode_reward += sum(reward.values())\n",
    "            total_steps += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        if total_steps % EVAL_INTERVAL == 0 and total_steps > 0:\n",
    "            score = evaluate(agents, parallel_env, episode, total_steps)\n",
    "            eval_scores.append(score)\n",
    "            eval_steps.append(total_steps)\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Create the 'data' directory if it doesn't exist\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "        # Save the files in the 'data' directory\n",
    "        np.save('data/ddpg_scores.npy', np.array(eval_scores))\n",
    "        np.save('data/ddpg_steps.npy', np.array(eval_steps))\n",
    "        episode_rewards.append(episode_reward)  # Store reward for this episode\n",
    "        for agent_name, rewards in agent_rewards.items():\n",
    "            mean_agent_reward = sum(rewards)\n",
    "            epsiode_mean_agent_rewards[agent_name].append(mean_agent_reward)\n",
    "\n",
    "    pbar.close()\n",
    "    smoothed_rewards = smooth_data(episode_rewards, window_size=100)  # Adjust the window size as needed\n",
    "\n",
    "    plot_average_episode_rewards(smoothed_rewards, scenario, output_dir)\n",
    "    plot_all_agents_rewards(epsiode_mean_agent_rewards, scenario, output_dir)\n",
    "    plot_all_agents_rewards_smooth(epsiode_mean_agent_rewards, scenario, output_dir)\n",
    "\n",
    "    return eval_scores\n",
    "\n",
    "\n",
    "def evaluate(agents, env, ep, step):\n",
    "    score_history = []\n",
    "    for i in range(3):\n",
    "        obs, _ = env.reset()\n",
    "        score = 0\n",
    "        terminal = [False] * env.max_num_agents\n",
    "        obs = list(obs.values())\n",
    "        while not any(terminal):\n",
    "            action = [agent.choose_action(obs[idx], eval=True)\n",
    "                      for idx, agent in enumerate(agents)]\n",
    "            action = {agent: act\n",
    "                      for agent, act in zip(env.agents, action)}\n",
    "\n",
    "            obs_, reward, done, truncated, info = env.step(action)\n",
    "            obs_ = list(obs_.values())\n",
    "            list_trunc = list(truncated.values())\n",
    "            list_reward = list(reward.values())\n",
    "            list_done = list(done.values())\n",
    "\n",
    "            terminal = [d or t for d, t in zip(list_done, list_trunc)]\n",
    "\n",
    "            obs = obs_\n",
    "            score += sum(list_reward)\n",
    "        score_history.append(score)\n",
    "    avg_score = np.mean(score_history)\n",
    "\n",
    "    return avg_score\n",
    "\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir = \"ddpg_plots\"\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "# Change this line to change the environment\n",
    "#  parallel_env, scenario = simple_tag_v3.parallel_env(max_cycles=25, continuous_actions=True, render_mode=\"rgb_array\"), \"predator_prey\"\n",
    "parallel_env, scenario =  simple_adversary_v3.parallel_env(N=1,max_cycles=25, continuous_actions=True, render_mode=\"rgb_array\"), \"Cooperative_Communication\"\n",
    "print(scenario)\n",
    "N_GAMES = 25_000\n",
    "# Create a subfolder with the name of the scenario\n",
    "scenario_dir = os.path.join(output_dir, scenario)\n",
    "if not os.path.exists(scenario_dir):\n",
    "    os.makedirs(scenario_dir)\n",
    "train_DDPG(parallel_env=parallel_env, N_GAMES=N_GAMES, scenario=scenario, output_dir=scenario_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
