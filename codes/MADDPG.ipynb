{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tick_range(values, interval=20):\n",
    "    start = np.floor(min(values) / interval) * interval\n",
    "    end = np.ceil(max(values) / interval) * interval\n",
    "    return np.arange(start, end + 1, interval)\n",
    "\n",
    "\n",
    "# Create a function to save plots\n",
    "def save_plot(plt, filename, output_dir):\n",
    "    if not os.path.exists(output_dir):  # Check if the output directory exists, create it if not\n",
    "        os.makedirs(output_dir)\n",
    "    plt.savefig(os.path.join(output_dir, filename))\n",
    "\n",
    "def plot_average_episode_rewards(average_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Episode Reward')\n",
    "    plt.title(f'Average Episode Reward Progress - {scenario}')\n",
    "    plt.plot(range(1, len(average_rewards) + 1), average_rewards)\n",
    "    plt.yticks(calculate_tick_range(average_rewards))\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Average_Episode_Reward_Progress_{scenario}.png', output_dir)\n",
    "    \n",
    "def plot_average_episode_rewards_rolling(average_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Episode Reward')\n",
    "    plt.title(f'Average Episode Reward Progress - {scenario}')\n",
    "    \n",
    "    plt.plot(range(1, len(average_rewards) + 1), average_rewards, alpha=0.3, label='Original')\n",
    "    \n",
    "    rewards_series = pd.Series(average_rewards)\n",
    "    smoothed_rewards = rewards_series.rolling(window=100).mean()\n",
    "\n",
    "    plt.plot(range(1, len(average_rewards) + 1), smoothed_rewards, color='red', label='Smoothed (Rolling Mean)')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(smoothed_rewards.dropna()))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Average_Episode_Reward_Progress_Rolling_Mean{scenario}.png', output_dir)\n",
    "\n",
    "def plot_all_agents_rewards(agent_rewards, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Agent Reward')\n",
    "    plt.title(f'All Agents Reward Progress (agent + avdversary) - {scenario}')\n",
    "    \n",
    "    all_rewards = np.concatenate([rewards for rewards in agent_rewards.values()])\n",
    "    for agent_name, rewards in agent_rewards.items():\n",
    "        plt.plot(range(1, len(rewards) + 1), rewards, label=f'Agent {agent_name}')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(all_rewards))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'All_Agents_Reward_Progress_{scenario}.png', output_dir)\n",
    "\n",
    "def plot_individual_agent_rewards(epsiode_mean_agent_rewards, agent_name, scenario, output_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Mean Agent Reward')\n",
    "    plt.title(f'Mean Agent Reward Progress - {scenario} (Agent {agent_name})')\n",
    "    plt.plot(range(1, len(epsiode_mean_agent_rewards) + 1), epsiode_mean_agent_rewards, label=f'Agent {agent_name}')\n",
    "    \n",
    "    plt.yticks(calculate_tick_range(epsiode_mean_agent_rewards))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    save_plot(plt, f'Individual_Agent_Reward_Progress_{scenario}_Agent_{agent_name}.png', output_dir)\n",
    "\n",
    "def plot_everything(output_dir, scenario, k, score_history_100, score_history, epsiode_mean_agent_rewards):\n",
    "    # Plot results for different subpolicies\n",
    "            output_subdir = os.path.join(output_dir, f'scenario_{scenario}', f'k_{k}')\n",
    "            os.makedirs(output_subdir, exist_ok=True)\n",
    "\n",
    "            # Plot for average episode rewards fancy\n",
    "            plot_average_episode_rewards(score_history_100, f\"{scenario} - {k}\", output_subdir)\n",
    "\n",
    "            # Plot for average episode rewards with rolling mean\n",
    "            plot_average_episode_rewards_rolling(score_history, f\"{scenario} - {k}\", output_subdir)\n",
    "            \n",
    "            # Plot for individual agent rewards\n",
    "            plot_all_agents_rewards(epsiode_mean_agent_rewards, f\"{scenario} - {k}\", output_subdir)\n",
    "            \n",
    "            # Plot for 'agent_0' only\n",
    "            agent_name = 'agent_0'\n",
    "            if agent_name in epsiode_mean_agent_rewards:\n",
    "                plot_individual_agent_rewards(epsiode_mean_agent_rewards[agent_name], agent_name, f\"{scenario} - {k}\", output_subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this class is used to store the experiences for each agent's subpolicy\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size, critic_dims, actor_dims, \n",
    "            n_actions, n_agents, batch_size,agent_names):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.n_agents = n_agents\n",
    "        self.actor_dims = actor_dims\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.agent_names = agent_names\n",
    "\n",
    "        self.state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, critic_dims))\n",
    "        self.reward_memory = np.zeros((self.mem_size, n_agents))\n",
    "        self.terminal_memory = np.zeros((self.mem_size, n_agents), dtype=bool)\n",
    "\n",
    "        self.init_actor_memory()\n",
    "\n",
    "    def init_actor_memory(self):\n",
    "        self.actor_state_memory = []\n",
    "        self.actor_new_state_memory = []\n",
    "        self.actor_action_memory = []\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            self.actor_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_new_state_memory.append(\n",
    "                            np.zeros((self.mem_size, self.actor_dims[i])))\n",
    "            self.actor_action_memory.append(\n",
    "                            np.zeros((self.mem_size, self.n_actions[i])))\n",
    "\n",
    "    # Store a new experience in the memory buffer\n",
    "    def store_transition(self, raw_obs, state, action, reward, \n",
    "                               raw_obs_, state_, done):\n",
    "        # circular buffer (when buffer is full, replace the old with new)       \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "\n",
    "        for agent_idx, agent_name in enumerate(self.agent_names):\n",
    "            self.actor_state_memory[agent_idx][index] = raw_obs[agent_name]\n",
    "            self.actor_new_state_memory[agent_idx][index] = raw_obs_[agent_name]\n",
    "            self.actor_action_memory[agent_idx][index] = action[agent_name]\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = [i for i in reward.values()]\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "\n",
    "    # Sample a batch of experiences from the memory buffer\n",
    "    def sample_buffer(self):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        # Randomly sample indices\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        states = self.state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        actor_states = []\n",
    "        actor_new_states = []\n",
    "        actions = []\n",
    "        # \n",
    "        for agent_idx in range(self.n_agents):\n",
    "            actor_states.append(self.actor_state_memory[agent_idx][batch])\n",
    "            actor_new_states.append(self.actor_new_state_memory[agent_idx][batch])\n",
    "            actions.append(self.actor_action_memory[agent_idx][batch])\n",
    "\n",
    "        return actor_states, states, actions, rewards, \\\n",
    "               actor_new_states, states_, terminal\n",
    "\n",
    "    # Check if the buffer has enough samples to start training\n",
    "    def ready(self):\n",
    "        if self.mem_cntr >= self.batch_size:\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Critic Network:\n",
    "# used to approximate the Q-value function given the state of the environment \n",
    "# and the actions of all agents for each agent\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, \n",
    "                    n_agents, n_actions, name, chkpt_dir):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.q = nn.Linear(fc2_dims, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.fc1(T.cat([state, action], dim=1)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.q(x)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def save_checkpoint(self,type):\n",
    "        checkpoint_temp = self.chkpt_file + type\n",
    "        os.makedirs(os.path.dirname(checkpoint_temp), exist_ok=True)\n",
    "        checkpoint_path = self.chkpt_file + \".pt\"  # Ensure the file has an extension\n",
    "        T.save(self.state_dict(), checkpoint_path)\n",
    "\n",
    "    \n",
    "    def load_checkpoint(self,type):\n",
    "        checkpoint_temp = self.chkpt_file + type\n",
    "        checkpoint_path = checkpoint_temp + \".pt\"\n",
    "        return \n",
    "        self.load_state_dict(T.load(checkpoint_path))\n",
    "\n",
    "# Actor Network:\n",
    "# used to approximate the policy function for each agent\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, \n",
    "                 n_actions, name, chkpt_dir):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.chkpt_file = os.path.join(chkpt_dir, name)\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "        self.pi = nn.Linear(fc2_dims, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    " \n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = T.softmax(self.pi(x), dim=1)\n",
    "\n",
    "        return pi\n",
    "\n",
    "    def save_checkpoint(self,type):\n",
    "        checkpoint_temp = self.chkpt_file + type\n",
    "        os.makedirs(os.path.dirname(checkpoint_temp), exist_ok=True)\n",
    "        checkpoint_path = checkpoint_temp + \".pt\"  # Ensure the file has an extension\n",
    "        T.save(self.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load_checkpoint(self,type):\n",
    "        checkpoint_temp = self.chkpt_file + type\n",
    "        checkpoint_path = checkpoint_temp + \".pt\"  # Ensure the file has an extension\n",
    "        self.load_state_dict(T.load(checkpoint_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Each agent has k instances of the subpolicy class\n",
    "class SubPolicy:\n",
    "    def __init__(self, actor_dims, critic_dims, n_actions, n_agents,  chkpt_dir,agent_name,\n",
    "                    alpha=0.01, beta=0.01, fc1=32,\n",
    "                    fc2=32, gamma=0.95, tau=0.01 ,\n",
    "                    ):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_actions = n_actions\n",
    "        self.agent_name = agent_name\n",
    "        self.actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions,\n",
    "                                  chkpt_dir=chkpt_dir,  name=self.agent_name+'_actor')\n",
    "        self.critic = CriticNetwork(beta, critic_dims,\n",
    "                            fc1, fc2, n_agents, n_actions,\n",
    "                            chkpt_dir=chkpt_dir, name=self.agent_name+'_critic')\n",
    "        self.target_actor = ActorNetwork(alpha, actor_dims, fc1, fc2, n_actions,\n",
    "                                        chkpt_dir=chkpt_dir,\n",
    "                                        name=self.agent_name+'_target_actor')\n",
    "        self.target_critic = CriticNetwork(beta, critic_dims,\n",
    "                                            fc1, fc2, n_agents, n_actions,\n",
    "                                            chkpt_dir=chkpt_dir,\n",
    "                                            name=self.agent_name+'_target_critic')\n",
    "        \n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # Ensure observation is a numpy array with standard numeric dtype\n",
    "        if isinstance(observation, list):\n",
    "            observations = np.array(observation)\n",
    "        else:\n",
    "            observations = observation\n",
    "\n",
    "        if observations.ndim == 1:\n",
    "            observations = observations.reshape(1, -1)\n",
    "\n",
    "        state = T.tensor(observations, dtype=T.float).to(self.actor.device)\n",
    "        actions = self.actor.forward(state)\n",
    "        min_v = (1 - actions).min()\n",
    "\n",
    "        # Ensure noise is generated on the same device as state\n",
    "        noise = (T.rand(self.n_actions, device=state.device) * min_v)\n",
    "    \n",
    "        action = actions + noise\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MADDPG:\n",
    "    def __init__(self, actor_dims, critic_dims,whole_state_observation_dims, n_agents, n_actions, env,\n",
    "                 scenario='simple',  alpha=0.01, beta=0.01, fc1=32,\n",
    "                 fc2=32, gamma=0.95, tau=0.01, chkpt_dir='tmp/maddpg/',k=1):\n",
    "        \n",
    "\n",
    "        # Create a memory buffer for each agent and each subpolicy\n",
    "        self.memory = [[MultiAgentReplayBuffer(1_000_000, whole_state_observation_dims, actor_dims,\n",
    "                                               n_actions, n_agents, batch_size=32,\n",
    "                                               agent_names=env.agents)\n",
    "                        for _ in range(k)] for _ in range(n_agents)]\n",
    "        self.k = k\n",
    "        self.agents = []\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        chkpt_dir += scenario\n",
    "        self.agents = {}\n",
    "        for agent_idx, agent_name in enumerate(env.possible_agents):\n",
    "            self.agents[agent_name] = Agent(actor_dims[agent_idx],\n",
    "                                            critic_dims,\n",
    "                                            n_actions[agent_idx], n_agents,\n",
    "                                            agent_name = agent_name,\n",
    "                                            alpha=alpha,\n",
    "                                            beta=beta,\n",
    "                                            chkpt_dir=chkpt_dir,k=k,agent_type=agent_name[0:-2])\n",
    "        \n",
    "        # Create a set of agent types : {'adversary', 'agent', etc..}\n",
    "        self.agent_types= set([agent.agent_type for agent in self.agents.values()])\n",
    " \n",
    "    def save_checkpoint(self,type):\n",
    "        print('... saving checkpoint ...')\n",
    "        for agent_name, agent in self.agents.items():\n",
    "            agent.save_models(type)\n",
    " \n",
    "    # def load_checkpoint(self,type):\n",
    "    #     print('... loading checkpoint ...')\n",
    "    #     for agent_name, agent in self.agents.items():\n",
    "    #         agent.load_models(type)\n",
    " \n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = []\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            action = agent.choose_action(raw_obs[agent_idx])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    " \n",
    "    def choose_action(self, raw_obs):\n",
    "        actions = {agent.agent_name: agent.choose_action(raw_obs[agent.agent_name]) for agent in self.agents.values()}\n",
    "        return actions\n",
    " \n",
    "    def randomly_choose_subpolicy(self):\n",
    "        # Generate random subpolicies for each agent type in one go\n",
    "        random_subpolicies = np.random.randint(0, self.k, size=len(self.agent_types))\n",
    "\n",
    "        # Create a mapping of agent types to their random subpolicy\n",
    "        self.type_to_subpolicy = dict(zip(self.agent_types, random_subpolicies))\n",
    "\n",
    "        # Update subpolicy for each agent only when accessed\n",
    "        for agent in self.agents.values():\n",
    "            agent.update_subpolicy(self.type_to_subpolicy[agent.agent_type])\n",
    "        \n",
    "    \n",
    "    def store_transition(self, obs, state, action, reward, obs_, state_, done):\n",
    "        # Store the transition in the memory buffer of the corresponding agent and active subpolicy:\n",
    "        # specified by the agent_idx and subpolicy_idx\n",
    "        for agent_idx, agent in enumerate(self.agents.values()):\n",
    "            self.memory[agent_idx][agent.current_subpolicy_idx].store_transition(obs, state, action, reward, obs_, state_, done)\n",
    "    \n",
    "    def learn(self):\n",
    "        for agent_idx,agent in enumerate(self.agents.values()):\n",
    "            agent.learn(self.memory[agent_idx][agent.current_subpolicy_idx],self.agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils_plots import plot_everything\n",
    "from maddpg import MADDPG\n",
    "from pettingzoo.mpe import simple_adversary_v3, simple_speaker_listener_v4, simple_spread_v3, simple_reference_v3, simple_tag_v3, simple_crypto_v3,simple_push_v3\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "\n",
    "def obs_list_to_state_vector(observation):\n",
    "    \"\"\"\n",
    "    Convert a list of observations to a state vector by concatenating them.\n",
    "    \"\"\"\n",
    "    state = np.array([])\n",
    "    for obs in observation:\n",
    "        state = np.concatenate([state, obs])\n",
    "    return state\n",
    "\n",
    "\n",
    "def visualize_agents(agents, env, n_episodes=20, speed=0.1):\n",
    "    \"\"\"\n",
    "    Visualize the agents' behavior in the environment.\n",
    "    \"\"\"\n",
    "    # Ensure speed is between 0 and 1\n",
    "    speed = np.clip(speed, 0, 1)\n",
    "\n",
    "    # Create a figure outside the loop\n",
    "    plt.figure()\n",
    "    for episode in range(n_episodes):\n",
    "        prev_reward = -np.inf\n",
    "        obs, _ = env.reset()\n",
    "        terminal = [False] * env.num_agents\n",
    "\n",
    "        while not any(terminal):\n",
    "            actions = agents.choose_action(obs)\n",
    "            obs, rewards, done, truncation, _ = env.step(actions)\n",
    "\n",
    "            # Sum rewards\n",
    "            rewards = sum(rewards.values())\n",
    "\n",
    "            # Determine direction\n",
    "            direction = \"Right direction\" if rewards > prev_reward else \"Wrong direction\"\n",
    "            prev_reward = rewards\n",
    "            \n",
    "            # Render as an RGB array\n",
    "            img = env.render()\n",
    "\n",
    "            # Clear the current axes and plot the new image\n",
    "            plt.clf()\n",
    "            plt.imshow(img)\n",
    "\n",
    "            # Determine the center position for the text\n",
    "            center_x = img.shape[1] / 2\n",
    "\n",
    "            # Add direction text to the figure, centered horizontally\n",
    "            plt.text(center_x, 20, direction, fontsize=12, color='white', bbox=dict(facecolor='black', alpha=1), ha='center')\n",
    "\n",
    "            # Display the updated figure\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            plt.pause(0.1 / speed)\n",
    "\n",
    "            terminal = [d or t for d, t in zip(done.values(), truncation.values())]\n",
    "\n",
    "        print(f'Episode {episode + 1} completed')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def solve_env_with_subpolicies(env, scenario, N_GAMES, evaluate, k_values=[1], plot=True, output_dir=None):\n",
    "    \"\"\"\n",
    "    Solve the environment using subpolicies.\n",
    "    \"\"\"\n",
    "    for k in k_values:\n",
    "        print(f\"Solving env {scenario} with k={k}\")\n",
    "        obs = env.reset()\n",
    "\n",
    "        n_agents = env.num_agents\n",
    "        actor_dims = [env.observation_spaces[agent_name].shape[0] for agent_name in env.agents]\n",
    "        n_actions = [env.action_spaces[agent_name].shape[0] for agent_name in env.agents]\n",
    "        critic_dims = sum(actor_dims) + sum(n_actions)\n",
    "        # what everyone is seeing\n",
    "        whole_state_observation_dims = sum(actor_dims)\n",
    "\n",
    "        maddpg_agents = MADDPG(actor_dims, critic_dims, whole_state_observation_dims, n_agents, n_actions,\n",
    "                               fc1=32, fc2=32,\n",
    "                               alpha=0.01, beta=0.01, scenario=scenario,\n",
    "                               chkpt_dir=f'tmp/maddpg/k_{k}/', env=env, k=k)\n",
    "\n",
    "        LOAD_TYPE = [\"Regular\", \"Best\"]  # Regular: save every 10k, Best: save only if avg_score > best_score\n",
    "        PRINT_INTERVAL = 500\n",
    "        SAVE_INTERVAL = 5000\n",
    "        MAX_STEPS = 25\n",
    "        total_steps = 0\n",
    "        score_history = []\n",
    "        score_history_100 = []\n",
    "        best_score = - np.inf  #the first score will always be better than this\n",
    "        epsiode_mean_agent_rewards = {agent_name: [] for agent_name in env.agents}\n",
    "        episode_lengths = []\n",
    "        \n",
    "        if evaluate:\n",
    "            maddpg_agents.load_checkpoint(LOAD_TYPE[0])  # load best\n",
    "            visualize_agents(maddpg_agents, env, n_episodes=5, speed=10)\n",
    "        else:\n",
    "            for i in tqdm(range(N_GAMES), desc=f\"Training with k={k}\"):\n",
    "                obs, _ = env.reset()\n",
    "                score = 0\n",
    "                done = [False] * n_agents\n",
    "                episode_step = 0\n",
    "                episode_length = 0\n",
    "                agent_rewards = {agent_name: [] for agent_name in env.agents}\n",
    "\n",
    "                # each episode, randomly choose a subpolicy\n",
    "                maddpg_agents.randomly_choose_subpolicy()\n",
    "                while not any(done):\n",
    "                    actions = maddpg_agents.choose_action(obs)\n",
    "\n",
    "                    obs_, reward, termination, truncation, _ = env.step(actions)\n",
    "                    state = np.concatenate([i for i in obs.values()])\n",
    "                    state_ = np.concatenate([i for i in obs_.values()])\n",
    "\n",
    "                    if episode_step >= MAX_STEPS:\n",
    "                        done = [True] * n_agents\n",
    "\n",
    "                    if any(termination.values()) or any(truncation.values()) or (episode_step >= MAX_STEPS):\n",
    "                        done = [True] * n_agents\n",
    "\n",
    "                    maddpg_agents.store_transition(obs, state, actions, reward, obs_, state_, done)\n",
    "\n",
    "                    if total_steps % 5 == 0:\n",
    "                        maddpg_agents.learn()\n",
    "\n",
    "                    obs = obs_\n",
    "                    for agent_name, r in reward.items():\n",
    "                        agent_rewards[agent_name].append(r)\n",
    "\n",
    "                    score += sum(reward.values())\n",
    "                    total_steps += 1\n",
    "                    episode_step += 1\n",
    "                    episode_length += 1\n",
    "                \n",
    "                score_history.append(score)\n",
    "                avg_score = np.mean(score_history[-100:])\n",
    "                score_history_100.append(avg_score)\n",
    "                episode_lengths.append(episode_length)\n",
    "\n",
    "                if (avg_score > best_score) and (i > PRINT_INTERVAL):\n",
    "                    print(\" avg_score, best_score\", avg_score, best_score)\n",
    "                    maddpg_agents.save_checkpoint(LOAD_TYPE[1])\n",
    "                    best_score = avg_score\n",
    "                if i % SAVE_INTERVAL == 0 and i > 0:\n",
    "                    maddpg_agents.save_checkpoint(LOAD_TYPE[0])\n",
    "\n",
    "                # Compute mean agent rewards\n",
    "                for agent_name, rewards in agent_rewards.items():\n",
    "                    mean_agent_reward = sum(rewards)\n",
    "                    epsiode_mean_agent_rewards[agent_name].append(mean_agent_reward)\n",
    "                  \n",
    "        if plot:\n",
    "            plot_everything(output_dir, scenario, k, score_history_100, score_history, epsiode_mean_agent_rewards)\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Specify the output directory\n",
    "output_dir = \"plots\"\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "env6, scenario6 = simple_adversary_v3.parallel_env(N=1, max_cycles=25, continuous_actions=True, render_mode='rgb_array'), \"Keep_Away\"\n",
    "envs = [env6]\n",
    "scenarios = [scenario6]\n",
    "k_values = [1]  # Add more values if needed\n",
    "for env, scenario in zip(envs, scenarios):\n",
    "    solve_env_with_subpolicies(env, scenario, N_GAMES=25_000, evaluate=False, k_values=k_values, plot=True, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
